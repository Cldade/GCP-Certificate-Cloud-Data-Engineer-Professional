{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in Data\n",
    "\n",
    "The data are gzipped CSV files. In Spark, these can be read directly using the textFile method and then parsed by splitting each row on commas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this cell Spark SQL is initialized and Spark is used to read in the source data as text and then returns the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile spark_analysis.py\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--bucket\", help=\"bucket for input and output\")\n",
    "args = parser.parse_args()\n",
    "BUCKET = args.bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a spark_analysis.py\n",
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "gcs_bucket='[Your-Bucket-Name]'\n",
    "spark = SparkSession.builder.appName(\"kdd\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "data_file = \"gs://\"+gcs_bucket+\"//kddcup.data_10_percent.gz\"\n",
    "raw_rdd = sc.textFile(data_file).cache()\n",
    "raw_rdd.take(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In cell In [5] each row is split, using , as a delimiter and parsed using a prepared inline schema in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a spark_analysis.py\n",
    "csv_rdd = raw_rdd.map(lambda row: row.split(\",\"))\n",
    "parsed_rdd = csv_rdd.map(lambda r: Row(\n",
    "    duration=int(r[0]),\n",
    "    protocol_type=r[1],\n",
    "    service=r[2],\n",
    "    flag=r[3],\n",
    "    src_bytes=int(r[4]),\n",
    "    dst_bytes=int(r[5]),\n",
    "    wrong_fragment=int(r[7]),\n",
    "    urgent=int(r[8]),\n",
    "    hot=int(r[9]),\n",
    "    num_failed_logins=int(r[10]),\n",
    "    num_compromised=int(r[12]),\n",
    "    su_attempted=r[14],\n",
    "    num_root=int(r[15]),\n",
    "    num_file_creations=int(r[16]),\n",
    "    label=r[-1]\n",
    "    )\n",
    ")\n",
    "parsed_rdd.take(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark analysis\n",
    "\n",
    "In cell In [6] a Spark SQL context is created and a Spark dataframe using that context is created using the parsed input data from the previous stage."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Row data can be selected and displayed using the dataframe's .show() method to output a view summarizing a count of selected fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a spark_analysis.py\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.createDataFrame(parsed_rdd)\n",
    "connections_by_protocol = df.groupBy('protocol_type').count().orderBy('count', ascending=False)\n",
    "connections_by_protocol.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In cell In [7] a temporary table (connections) is registered that is then referenced inside the subsequent SparkSQL SQL query statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a spark_analysis.py\n",
    "df.registerTempTable(\"connections\")\n",
    "attack_stats = sqlContext.sql(\"\"\"\n",
    "    SELECT\n",
    "      protocol_type,\n",
    "      CASE label\n",
    "        WHEN 'normal.' THEN 'no attack'\n",
    "        ELSE 'attack'\n",
    "      END AS state,\n",
    "      COUNT(*) as total_freq,\n",
    "      ROUND(AVG(src_bytes), 2) as mean_src_bytes,\n",
    "      ROUND(AVG(dst_bytes), 2) as mean_dst_bytes,\n",
    "      ROUND(AVG(duration), 2) as mean_duration,\n",
    "      SUM(num_failed_logins) as total_failed_logins,\n",
    "      SUM(num_compromised) as total_compromised,\n",
    "      SUM(num_file_creations) as total_file_creations,\n",
    "      SUM(su_attempted) as total_root_attempts,\n",
    "      SUM(num_root) as total_root_acceses\n",
    "    FROM connections\n",
    "    GROUP BY protocol_type, state\n",
    "    ORDER BY 3 DESC\n",
    "    \"\"\")\n",
    "attack_stats.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The last cell, In [8] uses the %matplotlib inline Jupyter magic function to redirect matplotlib to render a graphic figure inline in the notebook instead of just dumping the data into a variable. This cell displays a bar chart using the attack_stats query from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a spark_analysis.py\n",
    "%matplotlib inline\n",
    "ax = attack_stats.toPandas().plot.bar(x='protocol_type', subplots=True, figsize=(10,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a spark_analysis.py\n",
    "ax[0].get_figure().savefig('report.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a spark_analysis.py\n",
    "import google.cloud.storage as gcs\n",
    "bucket = gcs.Client().get_bucket(BUCKET)\n",
    "for blob in bucket.list_blobs(prefix='sparktodp/'):\n",
    "    blob.delete()\n",
    "bucket.blob('sparktodp/report.png').upload_from_filename('report.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a spark_analysis.py\n",
    "connections_by_protocol.write.format(\"csv\").mode(\"overwrite\").save(\n",
    "    \"gs://{}/sparktodp/connections_by_protocol\".format(BUCKET))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_list = !gcloud info --format='value(config.project)'\n",
    "BUCKET=BUCKET_list[0]\n",
    "print('Writing to {}'.format(BUCKET))\n",
    "!/opt/conda/miniconda3/bin/python spark_analysis.py --bucket=$BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud storage ls gs://$BUCKET/sparktodp/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud storage cp spark_analysis.py gs://$BUCKET/sparktodp/spark_analysis.py"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
